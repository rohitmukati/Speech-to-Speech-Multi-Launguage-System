# -*- coding: utf-8 -*-
"""Copy of Multimodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1021Y8Qk9qFtSua4QbxL6Z4JxT9VbyvWl
"""

!pip install gradio
!pip install git+https://github.com/huggingface/parler-tts.git

import gradio as gr
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from parler_tts import ParlerTTSForConditionalGeneration
import torch
import soundfile as sf
import os
from transformers import pipeline


# # Initialize Whisper model
# whisper_processor = WhisperProcessor.from_pretrained("yash072/Whisper-small-finetuned-hindi")
# whisper_model = WhisperForConditionalGeneration.from_pretrained("yash072/Whisper-small-finetuned-hindi")
# print("whisper loader for STT")


## loading wishper model
model_id = "yash072/Whisper_Smal_FineTuned_Hindi"  # update with your model id
pipe = pipeline("automatic-speech-recognition", model=model_id)


# Initialize IndicBERT model
indicbert_model = AutoModelForSequenceClassification.from_pretrained("ai4bharat/indic-bert")
indicbert_tokenizer = AutoTokenizer.from_pretrained("ai4bharat/indic-bert")
print("indic bert loader for Text to text ")
# Initialize Parler TTS
indic_parler_tts = ParlerTTSForConditionalGeneration.from_pretrained("ai4bharat/indic-parler-tts")
indic_parler_tokenizer = AutoTokenizer.from_pretrained("ai4bharat/indic-parler-tts")
description_tokenizer = AutoTokenizer.from_pretrained(indic_parler_tts.config.text_encoder._name_or_path)
print("text to speech")

# Device setup
device = "cuda:0" if torch.cuda.is_available() else "cpu"
indic_parler_tts = indic_parler_tts.to(device)

# def speech_to_text(audio_input):
#     # Convert speech to text using Whisper
#     audio_input, _ = sf.read(audio_input)  # Read the audio file
#     input_features = whisper_processor(audio_input, return_tensors="pt").input_features
#     predicted_ids = whisper_model.generate(input_features)
#     text = whisper_processor.decode(predicted_ids[0], skip_special_tokens=True)
#     return text

def transcribe_speech(filepath):
    output = pipe(
        filepath,
        max_new_tokens=256,
        generate_kwargs={
            "task": "transcribe",
            "language": "hi",
        },  # update with the language you've fine-tuned on
        chunk_length_s=30,
        batch_size=8,
    )
    return output["text"]

def process_query(text):
    # Use IndicBERT to process the text query
    inputs = indicbert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = indicbert_model(**inputs)
    response = torch.argmax(outputs.logits, dim=1).item()  # Get the most probable response class
    response_text = f"Predicted class: {response}"  # Placeholder response
    return response_text

def text_to_speech(response_text):
    # Convert text to speech using Indic Parler TTS
    prompt = response_text
    description = "A female speaker delivers a clear, natural tone in moderate speed and pitch."
    description_input_ids = description_tokenizer(description, return_tensors="pt").to(device)
    prompt_input_ids = indic_parler_tokenizer(prompt, return_tensors="pt").to(device)

    generation = indic_parler_tts.generate(
        input_ids=description_input_ids.input_ids,
        attention_mask=description_input_ids.attention_mask,
        prompt_input_ids=prompt_input_ids.input_ids,
        prompt_attention_mask=prompt_input_ids.attention_mask
    )
    audio_arr = generation.cpu().numpy().squeeze()

    # Save the output audio to disk
    output_file = "tts_output.wav"
    sf.write(output_file, audio_arr, indic_parler_tts.config.sampling_rate)

    return output_file

# Full pipeline function
def multimodal_pipeline(audio_input):
    # Step 1: Convert speech to text
    text_query = transcribe_speech(audio_input)

    # Step 2: Process the text query to get a response
    response_text = process_query(text_query)

    # Step 3: Convert the response text to speech and save audio
    audio_response_path = text_to_speech(response_text)

    return text_query, response_text, audio_response_path

# Create a Gradio interface for the multimodal pipeline
iface = gr.Interface(
    fn=multimodal_pipeline,
    inputs=gr.Audio(type="filepath", label="Record or Upload Audio"),
    outputs=[
        gr.Textbox(label="Transcribed Text"),
        gr.Textbox(label="LLM Response"),
        gr.Audio(label="Generated Audio Response")
    ],
    title="E-commerce Query Solution",
    description="Upload or record an audio query in Hindi to get a spoken response. Intermediate outputs such as transcribed text and LLM response are displayed."
)

# Launch the Gradio interface
iface.launch()

text_query = speech_to_text("/content/indic_tts_out2.wav")

text_query

from transformers import pipeline

model_id = "yash072/Whisper_Smal_FineTuned_Hindi"  # update with your model id
pipe = pipeline("automatic-speech-recognition", model=model_id)

def transcribe_speech(filepath):
    output = pipe(
        filepath,
        max_new_tokens=256,
        generate_kwargs={
            "task": "transcribe",
            "language": "hi",
        },  # update with the language you've fine-tuned on
        chunk_length_s=30,
        batch_size=8,
    )
    return output["text"]

text = transcribe_speech("/content/indic_tts_out2.wav")

print(text)







pip install git+https://github.com/huggingface/parler-tts.git

import gradio as gr
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
import soundfile as sf
import os

# Get your Hugging Face user access token
# Visit https://huggingface.co/settings/tokens to create one if you don't have it
# Replace 'YOUR_TOKEN' with your actual token
token = os.environ.get("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")

# Load the Whisper model for Speech-to-Text (Hindi)
whisper_processor = WhisperProcessor.from_pretrained("yash072/Whisper-small-finetuned-hindi")
whisper_model = WhisperForConditionalGeneration.from_pretrained("yash072/Whisper-small-finetuned-hindi")

# Load AI4Bharat IndicBERT model for Query Processing (Ensure model supports Hindi)
indicbert_model = AutoModelForSequenceClassification.from_pretrained("ai4bharat/indic-bert")
indicbert_tokenizer = AutoTokenizer.from_pretrained("ai4bharat/indic-bert")

# Load TTS model (Indic Parler)
indic_parler_tts = AutoModelForSeq2SeqLM.from_pretrained("ai4bharat/indic-parler-tts")
indic_parler_tokenizer = AutoTokenizer.from_pretrained("ai4bharat/indic-parler-tts")

def speech_to_text(audio_input):
    # Convert speech to text using Whisper (Force Hindi processing)
    audio_input, _ = sf.read(audio_input)  # Read the audio file
    input_features = whisper_processor(audio_input, return_tensors="pt").input_features
    predicted_ids = whisper_model.generate(input_features)
    text = whisper_processor.decode(predicted_ids[0], skip_special_tokens=True)
    return text

def process_query(text):
    # Ensure the input text for IndicBERT is in Hindi and the output is in Hindi
    inputs = indicbert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = indicbert_model(**inputs)
    # Instead of predicting classes, we process the response text directly
    response = "यह आपकी क्वेरी का उत्तर है: " + text  # Placeholder for actual processing logic
    return response

def text_to_speech(response_text):
    # Convert the response text back to speech using Indic Parler TTS
    input_ids = indic_parler_tokenizer(response_text, return_tensors="pt").input_ids
    audio_output = indic_parler_tts.generate(input_ids)
    audio = audio_output.squeeze(0).numpy()  # Convert to numpy array
    return audio

# Full pipeline function
def multimodal_pipeline(audio_input):
    # Step 1: Convert speech to text (in Hindi)
    text_query = speech_to_text(audio_input)

    # Step 2: Process the text query to get a response (ensure response is in Hindi)
    response_text = process_query(text_query)

    # Step 3: Convert the response text to speech
    audio_response = text_to_speech(response_text)

    return audio_response

# Create a Gradio interface for the multimodal pipeline
iface = gr.Interface(
    fn=multimodal_pipeline,
    inputs=gr.Audio(source="microphone", type="filepath"),
    outputs=gr.Audio(type="numpy"),
    live=True,
    title="E-commerce Query Solution",
    description="Speak a query in Hindi and get a spoken response in Hindi.",
)

# Launch the Gradio interface
iface.launch()

